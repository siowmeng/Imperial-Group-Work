---
title: "Analysis of IMDB Movie Dataset"
output: html_document
date: "16 October 2016"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(ggplot2)
library(ggthemes)
library(ggrepel)
library(reshape2)
library(dplyr)
library(plyr)
library(caret)
library(scales)
library(countrycode)
library(GGally)
library(knitr)
library(scales)
library(gridExtra)
library(pander)
library(stargazer)
```

#Executive Summary
*to be filled out after we have finished our analysis*

The purpose of this report is to try and understand what sort of factors contribute to the successfulness of a movie. The analyses is split into three types parts - descriptive, inferential and followed by an attempt to make a predictive model. The first section looks to analyze the data available and the second section aims to identify relevant variables which are correlated to gross revenue of a movie. The final section attempts to make a predictive model based on the chosen relevant variables and this model is tested against a subsetted test dataset, to see how accurate it is in predicting the gross revenue of a movie. 


From our analyses, we found that ...
- Budget has the most impact on revenue out of all the variables available in the dataset 
- Aside from budget, content total cast facebook likes, director facebook likes, ratingsm duration and key genres are also strong factors in determining whether a movie performs well in the box office or not
- Our predictive models are able predict the gross revenue with a Root-Mean-Square Error of 37-56 million USD, depending on the chosen variables in the particular model

However, there are many limitations to this dataset, including a limited number of variables and a clear bias in the number of English movies vs other language movies. Thus, the conclusions drawn from this analysis needs to be verified with a more extensive data set. Moving forward, it would be very useful to obtain data from other movie databases indigenious to countries other than the US and UK e.g. *http://www.bollywoodmdb.com* for Indian Bollywood movies. Though our model is able to predict revenues considerably well for a first attempt, and specifically for movies with revenue up to 50 million USD, it still has difficulties estimating the revenue with decent accuracy for movies with even higher gross revenues. Acquiring more data points and having datasets with relevant variables different from the current ones, can help to improve the accuracy of our predictive models.

#Introduction

With the advent of big data and advanced predictive analytics capabilities, very often, producers are trying to determine beforehand what will make a movie successful or fail. Rather than relying on intuition and guesswork, producers such as Netflix are trying to figure out what will make a movie a successful one by analysing historical data available in the movie industry and increase their success rates through the use of data analytics. For Netflix, the recent award-winning drama 'House of Cards' was a success story of theirs - descriptive, prescriptive and predictive analytics were crucial in the process of developing the drama, to gauge what content viewers were interested in.

Similarly, a question can be posed - whether one can predict the successfulness of a movie before it is released on cinema. Our key objective for this report is to determine what makes a movie popular (based on gross revenue). Descriptive analyses and influential analyses to look at which of the variables available in the dataset affect user ratings and gross revenue, will be followed by an attempt to develop a predictive model. This predictive model will try to determine which movies will be successful based on the relevant variable identified. 10% of the original data has been set aside to act as the test dataset and the model will be evaluated on its accuracy to predict gross revenue using Mean Squared Error and other success statistics.

#Methods

##Data

```{r, echo = FALSE}
movies <- read.csv(file = "movie_metadata.csv", header = TRUE, stringsAsFactors = FALSE, strip.white = TRUE)
```

The dataset used in this report was downloaded from *kaggle.com*, a website for data analysis competitions. The data was scraped from three websites using *Scrapy*, a Python library. The first website, *the-numbers.com*, is a website providing movie industry data. This website was used to scrape 5000 movie names along relevant data such as budget and gross domestic revenue. No pattern could be identified as to how the movie names were chosen. It was therefore assumed that the data represents a random sample. The scraped movie names were then matched with *imdb.com*, a popular resource for movie and TV-show ratings, and celebrity content, in order to get movie scores, direct links to movie pages, and other relevant features. All the movie and actor names were later aggregated and used to extract the number of Facebook likes on their respective official Facebook page. Finally, a face detection algorithm was applied to all movie posters to extract the number of faces in each poster. The dataset contains `r dim(movies)[1]` movies with `r dim(movies)[2]` variables spanning 100 years and 66 countries.    

Although *imdb.com* covers movies from various different countries, the website is only offered in English, which implies certain limitations. The movies included in the dataset are skewed towards an English-speaking audience. This means that movies from North America, the UK, and other English speaking countries are overrepresented. The dataset also includes more movies with release dates after 1980. In fact, multiple release years prior to 1980 include as little as one movie.
   
Despite these limitations the dataset provides a solid base for in-depth analysis. As a matter of fact, there exist few movie datasets that include such variety. This will allow the analysis to  reveal interesting insights and answer the report's leading question. The dataset can be retrieved [here](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset).

The following variables are included in the dataset:
```{r echo = FALSE}
# make table with column names and explanations

# create new df with column names ar rows
movie_variables <- colnames(movies)
movie_variables <- data.frame(sort(movie_variables))
# rename fisrt column
colnames(movie_variables) <- ("Variable Name")

actor_1_facebook_likes <- "Number of likes on main actor's Facebook page."
actor_1_name <- "Main actor's name."
actor_2_facebook_likes <- "Number of likes on first supporting actor's Facebook page."
actor_2_name <- "First supporting actor's name."
actor_3_facebook_likes <- "Number of likes on second supporting actor's Facebook page."
actor_3_name <- "Second supporting actor's name."
aspect_ratio <- "Aspect ratio the movie was shot in."
budget <- "Budget in USD. All budget's are estimates based on press reports."
cast_total_facebook_likes <- "Total number of likes of all actor Facebook pages."
color <- "Describes whether the movie was shot in color or black and white"
content_rating <- "Rating of suitability of movie for audience."
country <- "Country the movie was shot in. If shot in multiple countries the first IMDB entry was chosen. Throughout the analysis this variable serves as proxy for the country of origin."
director_facebook_likes <- "Number of likes on director's Facebook page."
director_name <- "Name of director."
duration <- "Movie length in minutes."
facenumber_in_poster <- "Number of faces on movie poster."
genres <- "Movie genre."
gross <- "Latest domestic gross revenue reported on the-number.com, in USD."
imdb_score <- "Score voted by IMDB users, from 1 to 10 (highest)."
language <- "Language in which movie was shot."
movie_facebook_likes <- "Number of likes on movie's official Facebook page."
movie_imdb_link <- "Link to movie page."
movie_title <- "Movie name."
num_critic_for_reviews <- "Number of critics that wrote a review."
num_user_for_reviews <- "Number of imdb users that wrote a review."
num_voted_users <- "Number of imdb users that rated the movie."
plot_keywords <- "Keywords describing the movie plot."
title_year <- "Movie release year."

# initiate new column for explanations
movie_variables$Explanation <- c(actor_1_facebook_likes,
                                 actor_1_name,
                                 actor_2_facebook_likes,
                                 actor_2_name,
                                 actor_3_facebook_likes,
                                 actor_3_name,
                                 aspect_ratio,
                                 budget,
                                 cast_total_facebook_likes,
                                 color,
                                 content_rating,
                                 country,
                                 director_facebook_likes,
                                 director_name,
                                 duration,
                                 facenumber_in_poster,
                                 genres,
                                 gross,
                                 imdb_score,
                                 language,
                                 movie_facebook_likes,
                                 movie_imdb_link,
                                 movie_title,
                                 num_critic_for_reviews,
                                 num_user_for_reviews,
                                 num_voted_users,
                                 plot_keywords,
                                 title_year)

# create table
kable(movie_variables, format = "markdown")
```

## Data Cleansing

An initial review of our dataset identified a few key issues to be addressed before progressing with the analysis:

* The existence of missing values in key variables
* The inclusion of columns unnecessary to our analysis
* The inclusion of erroneous special characters, which may be an artefact of text format conversion
* The inclusion of leading and trailing white spaces.

The below table shows the columns in the initial dataset that contain missing values:

```{r, echo = FALSE}
    ## function to calculate list of NAs within a column
colNA <- function(dfCol){ sum(is.na(dfCol)) }
## To show table of sum of NAs by column
na_values <- data.frame(colnames(movies), apply(movies, 2, colNA))
na_values <- na_values[na_values$apply.movies..2..colNA.>0, ]
colnames(na_values) <- c("Variable Name", "Number of missing values")
na_values <- na_values[order(na_values[, "Variable Name"]), ]
kable(na_values, format = "markdown", align = "l", row.names = FALSE)
```

*To discuss: since title year is not key to analysis, we shan't propose to remove the whole row altogether. Actually for this particular dataset, these records (with title year NA) has either gross=NA or budget=NA. So we can just remove the records which has budget=NA and gross=NA. -- Siow Meng*

To resolve these issues, the following cleansing process is applied:

* All rows where the title year has a missing value are removed - as there are a high number, and this variable is not expected to be key to the analysis.
* All rows where gross has a missing value are removed - as there are a high number, and this variable is expected to be one of the output variables
* All rows where the budget has a missing value are removed - as there are a high number, and this variable is expected to be key to the analysis.
* The aspect ratio column has a missing value removed - as there are a high number of NAs, and this variable is not expected to be key to the analysis. 
* The IMDB link column is removed - as this variable is not expected to be key to the analysis

```{r, echo = FALSE}
## To remove rows where NAs are present for any of the applicable columns
movies <- movies[complete.cases(movies[c("title_year","budget","gross")]),]
## To remove aspect ratio and imdb link columns
movies$aspect_ratio <- NULL
movies$movie_imdb_link <- NULL
```

* After this step there are only `r sum(is.na(movies))` remaining missing values within the dataframe. For these missing values, the mean value for that column will be used to replace all misisng values within that column. Although this has some drawbacks in terms of accuracy, it allows us to maintain rows of data with other valid information, and is an unbiased approach.

```{r, echo = FALSE}
## for loop to replace NAs with means for particular columns
for (i in c("actor_1_facebook_likes" , "actor_2_facebook_likes" , "actor_3_facebook_likes" , "num_critic_for_reviews" , "duration" , "facenumber_in_poster")){
    k <- which(colnames(movies) == i)
    movies[k][is.na(movies[k] == TRUE)] <- round(mean(movies[[k]], na.rm = TRUE), 0)
}
```

* Unwanted strings "Â" as well as leading and trailing white spaces are removed from the "title" column.

```{r, echo = FALSE}
movies <- movies[!duplicated(movies$movie_title),]
# Function to remove Â, leading and trailing whitespace from movies$movie_title
movie_title_processing <- function(str){
  str <- sub(pattern = "Â", replacement = "", str)
  str <- sub(pattern = "^\\s+|\\s+$", replacement ="", str)
}
# Apply previous function
movies$movie_title <- sapply(movies$movie_title, FUN = movie_title_processing)
```

Although some of these steps reduce the sample size for analysis, the rows removed would either cause later analysis to fail, cause the dataset to be inconsistent across various pieces of analysis, or produce misleading results.

This cleansed dataset (of `r nrow(movies)` rows) contains no NA values and is used for the remainder of the analysis. The head of the data frame is printed here:

*\@George, I agree with your comment to include the head, I have also included the scipen option below so that the budget does not show in scientific notation, but please let me know if you think this makes it look messier*

```{r, echo = FALSE, message = FALSE, results = "asis"}
set.seed(1)
options(scipen = 10000000)
intrain <- createDataPartition(y = movies[[1]], p = 0.9, list = FALSE)
train <- movies[intrain, ]
test <- movies[-intrain, ]

kable(t(movies[1:3, ]), format = "markdown")
```

10% of the cleansed dataset (`r nrow(test)`) is then set aside to be the "test" dataset, leaving the remaining 90%  of our dataset (`r nrow(movies)`) as the training dataset. The training dataset alone will be used for all descriptive, inferential and predictive analysis, including model building. The "test" dataset will be used at a later stage to check the accuracy of the predicted model.

#Theory

The exact factors that determine the gross revenue of a movie are not well known, which can be seen by the high variance of gross revenues of historical movie releases, even with respect to their budgets (explored further below). There are multiple mechanisms that determine the popularity, and the related gross revenue, of a movie.

If it is assumed that movie-makers (including producers, investors and executives), are well informed and profit-maximising, then one of the key predictors of movie revenue should be the budget. The mechanism for this is that a movie-maker should continue to invest money until the expected risk-adjusted returns are below the additional cost, so an increase in budget should lead to a corresponding increase in returns (gross revenue). However, not all movie-makers are necessarily profit maximising. Often there are other incentives to create movies such as recognition, prestigious awards (e.g. the Academy Awards), or an artistic mandate.

The gross revenue of a movie is also expected to be driven by its internal "quality", this is of course difficult to measure but proxies of critics and imdb scores and reviews can be used for quantitative analysis. This should be particularly relevant in the "information age" where the "quality" of a movie as measured by IMDB scores or critics reviews, can be shared amongst broad online and offline social networks, and can have a strong network effect to either increase or decrease viewership of a movie.

The popularity of a movie, in terms of ticket sales and therefore gross revenue, in theory should also be affected by the popularity of the cast and director, as "celebrity" culture would lead fans of those individuals to watch the movies that they act or direct in, and furthermore to promote them within their social circles. Therefore it is expected that there will be a positive correlation in the data between cast and director facebook likes, and the gross revenue.

As both movie-watching populations and ticket prices have been growing over time (see http://www.boxofficemojo.com/yearly/), it is expected that gross revenues of movies would also display growth over time, although this may not necessarily be the case as there are many other factors impacting the industry such as a growing number of theatrical releases for consumers to choose from (e.g. approx 4000 in 1920, and 165,329 in 2010, data from http://www.imdb.com/year/), and the rise in online streaming services and movie piracy.

It is also expected that PG-13 ratings attract a higher audience as they are technically open for all audiences (guidance is only recommended, not enforced, in the US) as per http://www.mpaa.org/film-ratings/, and as there are less rules than for a G/PG movie, their content generally targets a wider range of audiences.

Other key factors that influence the popularity of a movie are internal, such as the language, genre, and key elements of the plot. These factors have multiple potential mechanisms to impact on gross revenue: they directly impact the audience for the movie both in terms of size and demographics, they may also act as a proxy for the quality of movie, as there are different standards and expectations across genres for example, and they may be more directly linked to popular trends which may cause people to go to the cinema.

These factors are explored within this report, both individually and by considering their interrelationships, for the purpose of assessing their inclusion into a predictive model for gross revenue. 

#Analysis

##Descriptive Data Analysis

An initial correlation plot of all numerical variables displays the relationships between the variables in the dataset:

```{r, echo = FALSE, fig.width = 6, fig.align = "center"}
##extract only the numerical variables
moviesnumeric<- movies[c("duration", "title_year", "budget", "imdb_score", "movie_facebook_likes", "actor_1_facebook_likes", "actor_2_facebook_likes", "actor_3_facebook_likes", "cast_total_facebook_likes", "director_facebook_likes", "num_user_for_reviews", "num_critic_for_reviews", "facenumber_in_poster", "gross")]
## provide meaningful names to variables
labels_values <- rev(c("Gross", "# of faces in poster", "# of critics for reviews", "# of users for reviews", "Director FB likes",
                   "Total cast FB likes", "Actor 3 FB likes", "Actor 2 FB likes", "Actor 1 FB likes", "Movie FB likes",
                   "IMDB Score", "Budget", "Year of title", "Duration"))
##create correlation matrix
corrmatrix <- round(cor(moviesnumeric), 2)
meltedmovies<- melt(corrmatrix)

ggplot(data = meltedmovies, aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() + 
    labs(x = "", y = "", title = "Correlation Matrix") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_x_discrete(labels = labels_values) + 
    scale_y_discrete(labels = labels_values) + 
    theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), panel.border = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank())  +
    scale_fill_gradient2(low = "#ce1254", mid="#1C3A54", high = "#5BB2F5", midpoint = 0, limit = c(-1,1), space = "Lab", name="Pearson\nCorrelation")
```

A large proportion of the numerical varibles available are relating to the facebook likes of various entities. In theory, these variables are likely to be most relevant to the performance of movies only after the usage of Facebook was widespread for marketing and celebrities. The below correlation matrices show the different interelationship of variables following the facebook area (post 2005), and following the era where facebook was widely used for marketing and celebrities (post 2010):

```{r echo=FALSE, fig.width = 10, fig.align = "center"}
# Correlation plot for movies after 2005
corrmatrix2005 <- round(cor(moviesnumeric[moviesnumeric$title_year >= 2005, ], use="pairwise.complete.obs"), 2)
meltedmovies2005 <- melt(corrmatrix2005)

post2005plot <- ggplot(data = meltedmovies2005, aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() + 
    labs(x = "", y = "", title = "Correlation Matrix\n(Movies after 2005)") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    scale_x_discrete(labels = labels_values) + scale_y_discrete(labels = labels_values) +
    theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), panel.border = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank(), legend.position="none") +
    scale_fill_gradient2(low = "#ce1254", mid="#1C3A54", high = "#5BB2F5", midpoint = 0, limit = c(-1,1), space = "Lab", name="")  
    
# Correlation plot for movies after 2010
corrmatrix2010 <- round(cor(moviesnumeric[moviesnumeric$title_year >= 2010, ], use="pairwise.complete.obs"), 2)
meltedmovies2010 <- melt(corrmatrix2010)

post2010plot <- ggplot(data = meltedmovies2010, aes(x=Var1, y=Var2, fill=value)) +
    geom_tile() + 
    labs(x = "", y = "", title = "Correlation Matrix\n(Movies after 2010)") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    scale_x_discrete(labels = labels_values) + scale_y_discrete(labels = labels_values) +
    theme(axis.ticks.x = element_blank(), axis.ticks.y = element_blank(), panel.border = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank()) +
    scale_fill_gradient2(low = "#ce1254", mid="#1C3A54", high = "#5BB2F5", midpoint = 0, limit = c(-1,1), space = "Lab", name="")

##joint plot
grid.arrange(post2005plot, post2010plot, widths=c(2,2.4), ncol=2)
```

To additionally look at some key statistics regarding our output variable, the maximum is $`r format(max(movies$gross), big.mark= ",")`, the minimum is $`r format(min(movies$gross), big.mark= ",")`, and the standard deviation is $`r format(sd(movies$gross), big.mark= ",")`. This high standard deviation (and therefore variance), is expected as per the theory. The distribution and log distribution can be seen below, the first of which is positively skewed, and the second is negatively skewed.

```{r echo=FALSE}
##function to create human readable axis labels.
##code taken from publically available github to give human readable axis labels. (https://github.com/fdryan/R/blob/master/ggplot2_formatter.r)
human_numbers <- function(x = NULL, smbl =""){
  humanity <- function(y){             
    
    if (!is.na(y)){
      
       b <- round_any(abs(y) / 1e9, 0.1)
       m <- round_any(abs(y) / 1e6, 0.1)
       k <- round_any(abs(y) / 1e3, 0.1)
      
      if ( y >= 0 ){ 
        y_is_positive <- ""
      } else {
        y_is_positive <- "-"
      }
      
      if ( k < 1 ) {
        paste0(y_is_positive, smbl, y )
        } else if ( m < 1){
        paste0 (y_is_positive, smbl,  k , "k")
      } else if (b < 1){
        paste0 (y_is_positive, smbl, m ,"m")
      } else {
        paste0 (y_is_positive, smbl,  comma(b), "b")     
      }
    }
  }
  
  sapply(x,humanity)
}

human_num <- function(x){human_numbers(x, smbl = "")} 
human_usd <- function(x){human_numbers(x, smbl = "$")}
```


```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=10}
reg <- ggplot(movies, aes(x=gross)) + geom_histogram() + labs(x="Gross Revenue") + theme_few() + scale_x_continuous(label=human_num)

log <- ggplot(movies, aes(x=gross)) + geom_histogram() + scale_x_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), labels = c("1", "100", "10,000", "1m", "100m", "10bn"), limits = c(1, 1e+10)) + labs(x="Log (Gross Revenue)") + theme_few()

grid.arrange(reg,log,ncol=2)
```

The descriptive analysis below focuses on individual variables and relationships in turn, drawing both from the above correlations, and from relationships between numerican and non-numerical variables.

### Language  

The following chart illustrates the changes of average gross revenues from year 1920 to 2016. We can observe a large fluctuation between 1920 and 1968, this can be attributed to the lack of data during this period. The dataset only contains `r nrow(movies[movies$title_year <= 1968, ])` records from 1920 to 1968, whereas there are `r nrow(movies[movies$title_year > 1968, ])` records of movies screened after 1968.

```{r RevByYear, echo = FALSE, fig.width = 10, fig.align = "center"}
revYear <- ddply(movies, ~ title_year, summarise, meanRev = mean(gross, na.rm = TRUE))

ggplot(data = revYear, aes(x = title_year, y = meanRev)) + 
    geom_point(size = 2) +
    geom_line(size = 1) + 
    theme_bw() + 
    labs(title = "Average Gross Revenue from Year 1920 to 2016", x = "Year", y = "Average Gross Revenue") + 
    scale_x_continuous(breaks = seq(1920, 2016, 8), limits = c(1920, 2016)) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10),
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    annotation_logticks(sides = "lr", colour = "gray") 
```

The two charts below compare the gross revenues achieved by English and non-English movies across the years.  

For English movies, the huge fluctuations in early years are due to the small amount of movie data before year 1968. After 1968, the average gross revenue grew steadily to around US$80 million in year 2016.  

For non-English movies, we can observe fluctuations across the entire period (from 1920 to 2016). This is likely caused by the lack of non-English movie data. There are only `r nrow(movies[movies$language != "English", ])` non-English movies in the dataset, compared to `r nrow(movies[movies$language == "English", ])` English movies. In recent years (after year 1990), the average gross revenue of non-English movies range from below US$10 thousand to over US$100 million.

```{r RevByYearLanguage, echo = FALSE, fig.width = 10, fig.align = "center"}
revLanguage <- ddply(movies, ~ title_year + language, summarise, meanRev = mean(gross, na.rm = TRUE))
revLanguage$language[revLanguage$language != "English"] <- "Non-English"

ggplot(data = revLanguage, aes(x = title_year, y = meanRev, colour = language)) + 
    geom_point(size = 2) +
    geom_line(size = 1) + 
    theme_bw() + 
    labs(title = "Average Gross Revenue of Movies from Year 1920 to 2016", x = "Year", y = "Average Gross Revenue") + 
    scale_x_continuous(breaks = seq(1920, 2016, 8), limits = c(1920, 2016)) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10),
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    facet_grid(~ language) +
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 
```

Next, we separate the movie data into two groups: USA and non-USA. From the charts below, USA movies exhibit a similar pattern as the English movies. There is still a large fluctuations within the non-USA group. The number of USA movies clearly dominate this dataset.  

```{r RevByYearCountry, echo = FALSE, fig.width = 10, fig.align = "center"}
revCountry <- ddply(movies, ~ title_year + country, summarise, meanRev = mean(gross, na.rm = TRUE))
revCountry$country[revCountry$country != "USA"] <- "Non-USA"
revCountry$country <- factor(revCountry$country, levels = c("USA", "Non-USA"))

ggplot(data = revCountry, aes(x = title_year, y = meanRev, colour = country)) + 
    geom_point(size = 2) +
    geom_line(size = 1) + 
    theme_bw() + 
    labs(title = "Average Gross Revenue of Movies from Year 1920 to 2016", x = "Year", y = "Average Gross Revenue") + 
    scale_x_continuous(breaks = seq(1920, 2016, 8), limits = c(1920, 2016)) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10),
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    facet_grid(~ country) +
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 

```

This subsection inspects the effect of IMDB score on gross revenue. From the two plots below, it can be observed that gross revenue is positively correlated with IMDB score for all the four groups (i.e. USA, Non-USA, English, and non-English).  

From the first plot, both English and non-English movies have positive correlation between gross revenue and IMDB score. Note that non-English movies generally have much lower gross revenues and the y-axis of this plot is on logarithmic scale. As a result, English movies have stronger correlation than non-English movies (even though non-English movies seem to have higher slope). This is as expected since most (if not all) IMDB users are English speakers. A favourable IMDB score would reflect a higher popularity amongst English-speaking audiences. On the other hand, non-English movies might not be targeted to English-speaking populations. Hence a higher IMDB score might not reflect the true popularity of non-English movies.  

```{r GrossVSScoreEnglish, echo = FALSE, fig.width = 10, fig.align = "center"}

movies$english <- factor(movies$language == "English", levels = c(TRUE, FALSE), labels = c("English", "Non-English"))
movies$us_or_others <- factor(movies$country == "USA", levels = c(TRUE, FALSE), labels = c("USA", "Non-USA"))

ggplot(data = movies, aes(x = imdb_score, y = gross, colour = english)) + 
    geom_jitter(alpha = 0.1, width = 0.1) +
    theme_bw() +
    geom_smooth(method = "lm") + 
    labs(title = "Gross Revenue against IMDB Score for English and non-English Movies", 
         x = "IMDB Score", y = "Gross Revenue") + 
    scale_colour_discrete(name = "Languages") +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10),
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    annotation_logticks(sides = "lr", colour = "gray") 

```

We can observe from the second plot that there is a stronger correlation between gross revenue and IMDB score for USA movies. This is expected because IMDB is an US-based online service and its users are predominantly from the USA. A highly popular USA movie is expected to garner more favourable reviews from USA users.  

```{r GrossVSScoreUSA, echo = FALSE, fig.width = 10, fig.align = "center"}

ggplot(data = movies, aes(x = imdb_score, y = gross, colour = us_or_others)) + 
    geom_jitter(alpha = 0.1, width = 0.1) +
    theme_bw() +
    geom_smooth(method = "lm") + 
    labs(title = "Gross Revenue against IMDB Score for USA and non-USA Movies", x = "IMDB Score", y = "Gross Revenue") + 
    scale_colour_discrete(name = "Countries") +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10),
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    annotation_logticks(sides = "lr", colour = "gray") 

```

### Country

In this section, we look at whether the country of origin has a huge influence on:

1. The number of movies a country produces.
2. The revenue that a movie generates. 

Then, we check if there is any relationship between the gross revenue and IMDB score for countries with more than 10 movies.

```{r, echo = FALSE, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE}
# Sorting the movies by country and picking out those with more than 10 movies
counted <- count(movies, vars = "country")
removednas <- counted[complete.cases(counted), ]
sorted <- arrange(removednas, desc(freq))
topcountries <- sorted[1:12, ]
topcountries$country <- factor(topcountries$country, levels = topcountries$country[order(topcountries$freq)])

# Plot showing number of movies for the above countries
ggplot(data = topcountries, aes(x = country, y = freq, fill = country)) + 
    geom_bar(stat = "identity", colour = "black") +
    labs(title = "Number of movies for countries with more than 10 movies", x = "", y = "") + 
    geom_text(aes(label = freq), hjust = -0.2, vjust = 0.4) + 
    coord_flip() +
    theme_few() +
    theme(legend.position = "None") +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank()) 

# Subsetting the countries with 10 or more movies
countries <- subset(movies, country %in% c("Australia", "Canada", "China", "France", "Germany", "Hong Kong", "India", "Italy", "Japan", "Spain", "UK", "USA"))

# Plot of gross revenue for the above countries
gross <- countries$gross
ggplot(countries, aes(country, gross, fill = country)) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) + labs(title = "Gross revenue of movies by country", x = "", y = "Gross revenue") + theme_bw() + theme(legend.position = "None") + theme(axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(), axis.title.x = element_blank()) + scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) 

# Plot of gross revenue vs IMDB score for the above countries
score <- countries$imdb_score
ggplot(countries, aes(x = score, y = gross, colour = country)) + 
  geom_jitter() + scale_x_log10() + labs(title = "IMDB Score and Gross Revenue for different countries", x = "IMDB Score", y = "Gross Revenue") +
  facet_wrap(~ country) +
  theme_few() + 
  theme(legend.position = "None") + 
    scale_y_log10(breaks = c(1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("100", "10,000", "1 million", "100 millions", "1 billion")) +
    geom_jitter(alpha = 0.2) + 
    labs(title = "IMDB Score and Gross Revenue for different countries", x = "IMDB Score", y = "Gross Revenue") +
    facet_wrap(~ country, nrow = 2) +
    theme_bw() + 
    theme(legend.position = "None") + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    annotation_logticks(sides = "lr", colour = "gray") 
```

Looking at the barplot, we observe that the number of movies originating in USA is almost 10 times that of UK. Also, the most number of movies originate from Europe. On the other end, no more than 20 movies in our dataset originate from asia.

Next, we will look at the gross revenues for the countries with 10 or more movies. The boxplots indicate that the movies originating from these countries have a mean revenue of 1 million. All USA releases generate more than 10 million. At least 50% of movies from Australia, UK, Japan and Germany generate revenues of over 10 million. On the other hand, no movies from India, China and Italy generate more than 10 million.

Lastly, when we look at the scatterplots, we see that UK & US movies seem to be symmetrically about 'y = x' indicating a nearly positive correlation between IMDB score and gross revenue; however, the other countries show no particular relationship between the two variables.

*To discuss the interpretation of the above results: Should positive correlation be attributed to symmetricity around y=x? For other countries, there seem to be slight positive correlation, but the data is too few to have confident inference -- Siow Meng*

### Budget

For movie budget, it has been remarked by IMDB that the reported figures on the website are the 'negative costs' of a movie, so purely the production costs and excluding advertising and promotion costs. IMDB has also remarked that the budget costs are not very accurate and may actually be a estimate ballpark, as they can be very difficult to calculate and reported budgets may increase over time.

1. Budget of movies

2. Budget of films in the top 13 countries (with 10 or more movies in the data set, the rest have been omitted)

```{r, echo = FALSE, message = FALSE, fig.width = 10, fig.align = "center"}
##Histogram for movie budget
ggplot(data = movies, aes(x = budget)) + 
    geom_histogram(fill = "#FF9999", colour = "black") + 
    theme_bw() +
    scale_x_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) + 
    labs(x = "Budget (USD)", y = "Count", title = "Budget of Movies") +
    annotation_logticks(sides = "tb", colour = "gray") 

#movieswithinrange <- count(movies$budget > 1000000 & movies$budget < 100000000)


##Country vs budget boxplot

#Find out which countries with over 10 movies in data set
subsetcountry <- count(movies, vars = "country") 
subsetcountry <- subsetcountry[order(subsetcountry[,2], decreasing = FALSE),] 
subsetcountry$country <- factor(subsetcountry$country, levels = subsetcountry$country)
subsetcountry <- subsetcountry[subsetcountry[,2] >= 10,]


#Create dataframe with just Australia, Canada, China, France, Germany, Hong Kong, India, Italy, Japan, Mexico, Spain, UK, USA
budgetcountry <- subset(movies, country %in% c("Australia", "Canada", "China", "France", "Germany", "Hong Kong", "India", "Italy", "Japan", "Mexico", "Spain", "UK", "USA"))
budgetcountry <- subset(movies, country %in% subsetcountry$country)
budgetcountry$country <- factor(budgetcountry$country, levels = subsetcountry$country, ordered = TRUE)

#box plot for countries vs budget

ggplot(budgetcountry, aes(x = country, y = budget, fill = country)) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) + labs(x = "Country", y="Budget (USD)", title = "Budget of films by country")  +
    theme_bw() +  theme(legend.position="none") + 
    theme(axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(), axis.title.x = element_blank()) + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10))
```

From the histogram with count of movies across different budgets, we can see most movies are around the ballpark range of 1 million to 100 million, with around 85% of the movies falling under the range of 1 million to 100 million. A negative skew in the data can be observed. 

From the boxplots, we can see that the average budget on the films across different countries is similar, around a ballpark range of 10 million to 100 million. Mexico and Italy appears to have a lower average for film budget than the rest of ther other countries. (the data has been subsetted to include only countries with over 10 movies in the data set)

### Genres

Next, we will try to find out some information about the genres the movies belong to and some summary statistics for them. We must note that each movie may belong to more than one genre. In that case, each movie is counted as many times as the number of genres it belongs to. In the following graphs, we only present the most popular genres. The less popolar genres which include less than 10 movies have been omitted.

The first graph shows the number of movies per genre. Dramas and comedies are by far the most popular genres, followed by thriller, action and romance movies. 

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
genres <- c()
i <- 1
for (ins in movies$genres){
    g <- strsplit(ins, "[|]")
    for (gnr in g[[1]]){
        if (!(gnr %in% genres)){
            genres[i] <- gnr
            i = i + 1
        }
    }
}
# Create a dataframe with logical values which 
# indiacte the categories of each movie
movies$genres <- strsplit(movies$genres, "[|]")
genres_idx <- movies[, c("movie_title", "genres")]
i = 1
mat <- matrix(rep(0, (dim(movies)[1] * length(genres))), nrow = dim(movies)[1])
for (g in genres_idx$genres){
    idx <- which(genres %in% g)
    mat[i, idx] <- 1
    i = i + 1
}
colnames(mat) <- genres
movies_and_genres <- data.frame(mat)

# Find how many movies belong in each genre
sum <- rep(0, length(genres))
for (i in 1:length(genres)){
    sum[i] <- sum(movies_and_genres[, i])
}
genres_sum <- data.frame(genre = factor(genres), sum = sum)
genres_sum <- genres_sum[order(sum, decreasing = FALSE),]
genres_sum$genre <- factor(genres_sum$genre, levels = genres_sum$genre)
genres_sum <- genres_sum[genres_sum$sum > 10, ]

# Number of movies belonging to each genre
ggplot(genres_sum, aes(x = genre, y = sum, fill = genre)) + 
    geom_bar(stat = "identity", colour = "black") + 
    coord_flip() +
    labs(title = "Number of movies by genre", x = "", y = "") + 
    geom_text(aes(label = sum), hjust = -0.2, vjust = 0.4) + 
    theme_few() +
    theme(legend.position = "None") +
    theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank()) 
```

Next, we create a boxplot graph based on the genres to observe how gross revenue is distributed among different genres. We observe great differences in the distribution of gross revenue in each genre, with Animation movies being the most profitable and Documentaries the least profitable on average.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
## Create an appropriate dataframe with gross, imdb_score and genres for each movie
movies_and_genres <- cbind(gross = movies$gross, score = movies$imdb_score, movie_title = movies$movie_title, movies_and_genres, stringsAsFactors = FALSE)
## saving a full wide data frame to be used in later analysis
movies_and_genres_wide <- movies_and_genres
movies_and_genres <- melt(movies_and_genres, id = c("gross", "score", "movie_title"))
movies_and_genres$variable <- gsub("[.]", " ", movies_and_genres$variable)
movies_and_genres <- movies_and_genres[movies_and_genres$value == 1, ] 
movies_and_genres$value <- NULL
colnames(movies_and_genres) <- c("gross", "score", "movie_title", "genre")
movies_and_genres$genre <- factor(movies_and_genres$genre, levels = genres_sum$genre)
movies_and_genres <- movies_and_genres[complete.cases(movies_and_genres), ]

# Boxplot of genres and profit
ggplot(movies_and_genres, aes(genre, gross, fill = genre)) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) +
    coord_flip() +
    labs(title = "Gross revenue of movies by genre", x = "", y = "") + 
    theme_bw() +
    theme(legend.position = "None") +
    theme(axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10))
```

Finally, we observe the diffirences between genres based not only on their gross revenue but also on theit IMDB score. 

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# Scatter plots of genres based on gross and imdb score
ggplot(movies_and_genres, aes(x = score, y = gross, colour = genre)) + 
    geom_jitter(alpha = 0.1) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    labs(title = "Gross Revenue and IMDB score for different genres", y = "Gross Revenue", x = "IMDB Score") +
    facet_wrap(~ genre) +
    theme_bw() + 
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 
```

#### Plot Keywords

We perform a similar analysis to the previous one, but this time based on plot keywords. Again, each movie may have more than one keywords that describe its plot. However, if one movie has more than one keywords, we will count it as many times as the keywords used to describe it. To simplify things, we will only look at the 20 most popular keywords, as the vast majority of all the possible keywords appear only a few times in the whole dataset and doesn't provide us with useful insights. As we see, the most popular keywords are love, friend, murder and death, followed by some less usual ones.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
movies0 <- movies[movies$plot_keywords != "", ]
keywords <- c()
i <- 1
for (ins in movies0$plot_keywords){
    kw <- strsplit(ins, "[|]")
    if (length(kw) != 0){
        for (word in kw[[1]]){
            if (!(word %in% keywords)){
                keywords[i] <- word
                i = i + 1
            }
        }
    }
}
# Create a dataframe with logical values which 
# indiacte the keywords of each movie
movies0$plot_keywords <- strsplit(movies0$plot_keywords, "[|]")
keywords_idx <- movies0[, c("movie_title", "plot_keywords")]
i = 1
mat <- matrix(rep(0, (dim(movies0)[1] * length(keywords))), nrow = dim(movies0)[1])
for (word in keywords_idx$plot_keywords){
    idx <- which(keywords %in% word)
    mat[i, idx] <- 1
    i = i + 1
}
colnames(mat) <- keywords
movies_and_keywords <- data.frame(mat)

# Find how many movies belong in each keyword
sum <- rep(0, length(keywords))
for (i in 1:length(keywords)){
    sum[i] <- sum(movies_and_keywords[, i])
}
keywords_sum <- data.frame(keywords = factor(keywords), sum = sum)
keywords_sum <- keywords_sum[order(sum, decreasing = FALSE),]
keywords_sum$keywords <- factor(keywords_sum$keywords, levels = keywords_sum$keywords)
#keywords_sum <- keywords_sum[keywords_sum$sum > 39, ]
keywords_sum <- keywords_sum[(dim(keywords_sum)[1]-19):dim(keywords_sum)[1] ,]

# Number of most popular keywords
ggplot(keywords_sum, aes(x = keywords, y = sum, fill = keywords)) + 
    geom_bar(stat = "identity", colour = "black") + 
    coord_flip() +
    labs(title = "Number of movies by keyword", x = "", y = "") + 
    geom_text(aes(label = sum), hjust = -0.2, vjust = 0.4) + 
    theme_few() +
    theme(legend.position = "None") +
    theme(axis.text.x=element_blank(), axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),
          axis.title.x = element_blank(), axis.title.y = element_blank()) 
```

Same as before, we create a boxplot graph based on the keywords to observe how gross revenue is distributed among the most popular plot keywords. We observe that the differences in the distribution of gross revenue among keywords are not that great in comparison with those based on genres. Interestingly, however, it seems that the most popular keywords are not the ones that bring the greatest gross revenue, while somee less populat keywords appear to bring greatest gross revenue on average.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
## Create an appropriate dataframe with gross, imdb_score and keywords    for each movie
movies_and_keywords <- cbind(gross = movies0$gross, score = movies0$imdb_score, movies_and_keywords, stringsAsFactors = FALSE)
movies_and_keywords <- melt(movies_and_keywords, id = c("gross", "score"))
movies_and_keywords$variable <- gsub("[.]", " ", movies_and_keywords$variable)
movies_and_keywords <- movies_and_keywords[movies_and_keywords$value == 1, ] 
movies_and_keywords$value <- NULL
colnames(movies_and_keywords) <- c("gross", "score", "keywords")
movies_and_keywords$keywords <- factor(movies_and_keywords$keywords, levels = keywords_sum$keywords)
movies_and_keywords <- movies_and_keywords[complete.cases(movies_and_keywords), ]

# Boxplot of keywords and profit
ggplot(movies_and_keywords, aes(keywords, gross, fill = keywords)) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) +
    coord_flip() +
    labs(title = "Gross revenue of movies by keyword", x = "", y = "") + 
    theme_bw() +
    theme(legend.position = "None") +
    theme(axis.ticks.y = element_blank(), panel.grid.major.y = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank()) + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10))
```

Again, we observe the diffirences between plot keywords based not only on their gross revenue but also on theit IMDB score. 

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# Scatter plots of keywords based on gross and imdb score
ggplot(movies_and_keywords, aes(x = score, y = gross, colour = keywords)) + 
    geom_jitter(alpha = 0.2) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    labs(title = "Gross Revenue and IMDB score for different keywords", y = "Gross Revenue", x = "IMDB Score") +
    facet_wrap(~ keywords, nrow = 4) +
    theme_bw() + 
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 
```

*Should we move the "IMDB score of movies by keyword" plot to appendix? -- Siow Meng*

### Gross revenue over time
```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# code to create continent column
movies$continent <- countrycode(as.character(movies$country), "country.name", "continent")

# divide "Americas" into "North Americas" and "South America"
south_America = c("Brazil", "Argentina", "Chile", "Colombia", "Peru")

for (i in 1:nrow(movies)) {
  if (is.na(movies$continent[i])) {
    next
  } else if ((movies$continent[i] == "Americas") & (movies$country[i] %in% south_America)) {
    movies$continent[i] <- "South America"
  } else if ((movies$continent[i] == "Americas") & (!(movies$country[i] %in% south_America))) {
    movies$continent[i] <- "North America"
  } 
}

```


```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# create function that converts years into decades
convert_decade <- function(year){
  low <- year - year %% 10
  high <- year - year %% 10 + 9
  paste(as.character(low), as.character(high), sep = "-")
}
# apply previous function
movies$decade <- sapply(movies$title_year, FUN = convert_decade)
```

Whereas in the past, movies were the principal source of entertainment, there are today several other sources to consider. While they are not consumed in the same way, they can be considered serious competitors to the classic movie. Some of the sources are high budget TV-Shows, video games, and user created content to name a few. It is therefore of interest to analyse the development of gross revenue over time.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# plot gross vs. years
temp <- movies[!is.na(movies$title_year),]
ggplot(temp, aes(x = factor(title_year), y = gross, fill = factor(title_year))) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) +
    theme_bw() + 
    theme(axis.text.x=element_text(angle = 45, hjust = 0.5, vjust = 0.5), legend.position = "None", axis.title.x = element_blank()) +
    labs(title = "Gross revenue against year", x = "", y = "Gross Revenue") + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billion"), limits = c(1, 1e+10)) +
    scale_x_discrete(breaks = seq(1920, 2016, 2), labels = seq(1920, 2016, 2), drop = FALSE) +
    annotation_logticks(sides = "lr", colour = "gray") 

```

The above plot shows gross revenue against from 1920 to 2016. Overall the plot shows a slight upward trend. However, there are two sections with distinct behaviors. The first section includes movies from 1920 to 1972 and shows a big variance between mean gross revenue for different years. The second section includes movies from 1972 to onwards. This section shows a smaller variance in mean gross revenue for different years. In addition, the plot has more outliers with low revenues than outliers with high revenues. Several release years, however, include fewer than ten movies and can therefore not be considered representative of a year's gross revenue. To get a more accurate picture the following two plots remove years with less than ten entries.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
# plot all years that have at least ten movies
# table(movies$title_year)  
# shows that from 1980 onwards all the years have at least ten entries

# subset movies data frame to include all movies from 1980 onwards
temp <- movies[movies$title_year >= 1980, ]
# plot the range of gross revenues for all years
p1 <- ggplot(temp, aes(x = factor(title_year), y = gross, fill = factor(title_year))) + 
    geom_boxplot() +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5), legend.position = "None", axis.title.x = element_blank()) +
    labs(title = "Gross revenue against year, for years with >= 10 movies", x = "", y = "Gross Revenue") +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "1 billions"),  limits = c(1, 1e+10)) +
   annotation_logticks(sides = "lr", colour = "gray") 

lm_model2 <- lm(log10(gross) ~ title_year, data = temp)

# make a scatter plot that shows trend in gross revenues

p2 <- ggplot(temp, aes(x = title_year, y = gross, colour = factor(title_year))) + 
    geom_jitter(alpha = 0.4) +
    geom_abline(intercept = lm_model2$coefficients[1], slope = lm_model2$coefficients[2], size = 1) +
    theme_bw() + 
    labs(x = "", y = "Gross Revenue") +
    theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5), legend.position = "None", axis.title.x = element_blank()) +
    scale_x_continuous(breaks = seq(min(temp$title_year), max(temp$title_year), by = 1), 0.5) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    annotation_logticks(sides = "lr", colour = "gray") 

grid.arrange(p1, p2, ncol=1)
```

The plots now show a different overall picutre. Compared to the first plot they cannot be visually divided into two distinct sections. In fact, the trendline on the second plot shows that revenues are on a slight downward trend. 

*The average gross revenues seem to be growing slightly in recent years (first plot in language section). The growth might seem small but the y-axis is in logarithmic scale so it's significant. --Siow Meng*

```{r, echo = FALSE}
# create function that converts years into decades
# function left here because used by Louise
convert_decade <- function(year){
  low <- year - year %% 10
  high <- year - year %% 10 + 9
  paste(as.character(low), as.character(high), sep = "-")
}
# apply previous function
movies$decade <- sapply(movies$title_year, FUN = convert_decade)
```

###Directors

```{r echo = FALSE, message = FALSE}
##create summary statistics for average imdb score & number of movies for each director
directorssummary <- ddply(movies, ~ director_name,summarise,score_average=round(mean(imdb_score),2), gross_average = round(mean(gross),2), number_of_movies=length(director_name), director_facebook_likes = max(director_facebook_likes))
##sort by # of movies then imdb average score
sorteddirectorsummary <- arrange(directorssummary, desc(number_of_movies), desc(score_average))
##ensure that factors are in the order of number of movies, otherwise ggplot will default to alphabetical ordering in the graph
sorteddirectorsummary$director_name <- factor(sorteddirectorsummary$director_name, levels = sorteddirectorsummary$director_name[order(sorteddirectorsummary$number_of_movies)])
```


```{r in-text-fig, echo=FALSE,  fig.width = 3, fig.height=2.5, out.extra='style="float:right"'}
##Histogram for number of moveies per director
ggplot(sorteddirectorsummary, aes(x = number_of_movies)) + 
    geom_histogram(binwidth = 1, fill = "darkblue") + 
    theme_bw() +
    labs(x = "Number of movies\nper director", y = "Number of directors",
         title = "Density of number of \nmovies per director") +
    scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30), limits = c(0, 30)) 
##
```

```{r echo=FALSE, fig.width = 10, fig.align = "center"}
##add a categorical variable for one movie or more
sorteddirectorsummary$more_than_one_movie <- rep.int(0, nrow(sorteddirectorsummary))
sorteddirectorsummary$more_than_one_movie[sorteddirectorsummary$number_of_movies > 1] <- 1
##calculate the percentage of directors with exactly one movie
dirpercentage <- round(100 * length(sorteddirectorsummary$number_of_movies[sorteddirectorsummary$number_of_movies == 1])
                       / length(sorteddirectorsummary$number_of_movies), 2)
```

Before looking into the relationship with the key output variable, gross revenue, descriptive statistics of the director data are explored. There are `r nrow(sorteddirectorsummary)` distinct directors in the data sample, the density of movies per director is shown on the figure on the right.

This is clearly positively skewed, with many directors having only 1 movie in the dataset, and only a few having significantly more. In fact, approximately  `r dirpercentage`% of those directors have only 1 movie in the top 5000, leaving `r 100-dirpercentage`% that have more than 1.

```{r echo=FALSE}
##add # of movies per director to the main dataset.
movieswithdirectordata <- merge(movies, sorteddirectorsummary, by = "director_name")
movieswithdirectordata <- rename(movieswithdirectordata, c('number_of_movies' = 'dir_number_of_movies'))
```

```{r echo = FALSE, message = FALSE, fig.width = 3, fig.height = 2.5, fig.align = "center", out.extra='style="float:left"'}
##plot for gross revenue vs number of directors in sample
ggplot(movieswithdirectordata, aes(x = dir_number_of_movies, y = gross)) + 
    geom_jitter(alpha = 0.1, width = 1) +
    theme_bw() +
    labs(x = "Number of movies\nper director", y = "Gross Revenue") + 
    scale_y_continuous(label = human_usd) + 
    scale_x_continuous(breaks = c(0, 5, 10, 15, 20, 25, 30), limits = c(0, 30)) +
    geom_smooth()
##*Shall I convert the y-axis of the following graphs into log scale? I also set the limits of x-axis to [0, 30] and we miss 1 observation with x value >100. --George*##
```

An initial look at the relationship between the number of movies that the director has in the dataset, and the gross revenue of those movies does not show a clearly defined correlation, which can be seen in the figure on the left.


To check against one potential confounder for the relationship that number of movies per director has with gross, the correlation between movies per director and budget is calculated.

```{r echo=FALSE}
c <- cor.test(movieswithdirectordata$dir_number_of_movies, movieswithdirectordata$budget)
pander(c, caption="")
```
*Do we use percentage sign to display correlation value? -- Siow Meng*

Even though this correlation was found to be statistically significant, with a p-value of `r c$p.value`, it is quite small (`r round(cor(movieswithdirectordata$dir_number_of_movies, movieswithdirectordata$budget), 3)`%), so the analysis of movies by director sample on gross revenue can be continued without an obvious confounder of budget.

Focusing on the relationship between gross revenue and number of movies per director with the additional dimension of genres, there is an approximate pattern of a positive correlation up to 10 movies by director, and then a less predictable path for those with directors with higher numbers of movies.

```{r echo = FALSE}
movies_and_genres_withdirectordata <- merge(movies_and_genres, movieswithdirectordata[,c("movie_title","dir_number_of_movies")], by = "movie_title")
```

```{r echo = FALSE, fig.width = 10, fig.align = "center", warning = FALSE, message = FALSE}
##requires the above
ggplot(movies_and_genres_withdirectordata, aes(x = dir_number_of_movies, y = gross, colour = genre)) + 
  geom_jitter(alpha = 0.1) + 
    scale_y_continuous(label=human_usd) + 
    labs(title = "Gross Revenue and # of director movies for different genres", x = "# of Movies by Director", 
         y = "Gross Revenue") + 
    facet_wrap(~ genre) + 
    theme(legend.position = "None") + geom_smooth(color="#000000")
```



```{r echo = FALSE, fig.width = 10, fig.align = "center"}
##add a categorical variable for ten movies or more
sorteddirectorsummary$more_than_ten_movies <- rep("Fewer than 10", nrow(sorteddirectorsummary))
sorteddirectorsummary$more_than_ten_movies[sorteddirectorsummary$number_of_movies>10] <- "10+"
##calculate the percentage of directors with  more than 10 movies
morethantendirpercentage <- round(100*length(sorteddirectorsummary$number_of_movies[sorteddirectorsummary$more_than_ten_movies == "10+"])/length(sorteddirectorsummary$number_of_movies),2)
##merge movie data with director data
movieswithdirectordata <- merge(movieswithdirectordata, sorteddirectorsummary[,c("director_name","more_than_ten_movies")], by = "director_name")
##refactor so that fewer than 10 is before 10+
movieswithdirectordata$more_than_ten_movies <- factor(movieswithdirectordata$more_than_ten_movies, levels = c("Fewer than 10", "10+"))
##calculate the percentage of movies where the director has more than ten movies
morethantenmovpercentage <- round(100*length(movieswithdirectordata$more_than_ten_movies[movieswithdirectordata$more_than_ten_movies == "10+"])/length(movieswithdirectordata$more_than_ten_movies), 2)
```
Only approximately `r morethantendirpercentage`% directors have more than ten movies, which, when considering movies, constitutes approximately `r morethantenmovpercentage`% of movies in our sample.

# Inferential Data Analysis

*For now, each have a section with one comparison and test/set of tests (t.test, var.test, cor.test etc.), it is expected this section will also include plots, later we can discuss how this flows into a narrative - we should let everyone in the slack group know what we're doing so that we don't overlap unecessarily*

*It is expected that these will be related to how inputs are correlated with the two outcome variables (gross and imdb_score)*

### Language and Country  

*The boxplots below are drawn on normal scale since we'll want to visualize the huge variance difference between English and non-English movies. The coord_cartesian is used to set the y range while not discarding the values outside of this range.*  

From the descriptive analysis section, we learned that the English movies (in this dataset) are generally more popular than non-English movies. In this section, we shall perform hypothesis testing to check if there is any statistically significant difference in the average gross revenues achieved by English and non-English movies.  

```{r englishtest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
# boxplot of English & Non-English movies

movies$english <- factor(movies$language == "English", levels = c(TRUE, FALSE), labels = c("English", "Non-English"))
movies$us_or_others <- factor(movies$country == "USA", levels = c(TRUE, FALSE), labels = c("USA", "Non-USA"))

ggplot(data = movies, aes(x = english, y = gross, fill = english)) + 
    theme_bw() + 
    geom_boxplot() + 
    stat_summary(fun.y = mean, colour = "red", geom = "point", size = 3, show.legend = FALSE) + 
    labs(title = "Gross Revenue of English and non-English Movies", 
         x = "", y = "Gross Revenue") + 
    scale_y_continuous(breaks = c(0, 15e+06, 30e+06, 45e+06, 60e+06, 75e+06), 
                       labels = c("0", "15 millions", "30 millions", "45 millions", 
                                  "60 millions", "75 millions")) + 
    coord_cartesian(ylim = c(0, 80e6)) + 
    theme(legend.position = "none", axis.ticks.x = element_blank(), 
          panel.grid.major.x = element_blank(), axis.title.x = element_blank())
```

From the above boxplot, we can observe a great difference between the revenues achieved by English and non-English movies. More than 75% of the non-English movies achieved gross revenue of US$10 million or lower. In contrast, more than half of the English movies have more than US$30 million revenues.  

The below table tabulates the t-test results. The results show that the average gross revenues of English movies are significantly higher than non-English movies.  

```{r englishmeantest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
pander(t.test(movies$gross[movies$language == "English"], movies$gross[movies$language != "English"], alternative = "greater"), caption = "")
```

In addition, the gross revenues of English movies vary greatly (wider range of gross revenues), compared to non-English movies. The below variance test output confirms this.  

```{r englishvartest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
pander(var.test(movies$gross[movies$language == "English"], movies$gross[movies$language != "English"], alternative = "greater"), caption = "")
```

In summary, we are confident to say that the English movies in this dataset are generally more popular than non-English movies.  

Next, we will focus on the differences between USA and non-USA movies. The boxplot below shows a similar pattern. Movies shot in the USA have achieved more than US$50 million in average gross revenue whereas the average gross revenue of non-USA movies is around US$25 million.  

```{r usatest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
# boxplot of USA & Non-USA movies

ggplot(data = movies, aes(x = us_or_others, y = gross, fill = us_or_others)) + 
    theme_bw() + 
    geom_boxplot() + 
    stat_summary(fun.y = mean, colour = "red", geom = "point", size = 3, show.legend = FALSE) + 
    labs(title = "Gross Revenue of USA and non-USA Movies", 
         x = "", y = "Gross Revenue") + 
    scale_y_continuous(breaks = c(0, 15e+06, 30e+06, 45e+06, 60e+06, 75e+06), 
                       labels = c("0", "15 millions", "30 millions", "45 millions", 
                                  "60 millions", "75 millions")) + 
    coord_cartesian(ylim = c(0, 80e6)) + 
    theme(legend.position = "none", axis.ticks.x = element_blank(), 
          panel.grid.major.x = element_blank(), axis.title.x = element_blank())
```

The below table tabulates the t-test results. The results show that the average gross revenues of USA movies are significantly higher than non-USA movies.  

```{r usameantest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
pander(t.test(movies$gross[movies$country == "USA"], movies$gross[movies$country != "USA"], alternative = "greater"), caption="")
```

In addition, the gross revenues of USA movies vary greatly (wider range of gross revenues), compared to non-USA movies. The below variance test output confirms this.  

```{r usavartest, echo = FALSE, results = TRUE, fig.width = 10, fig.align = "center"}
pander(var.test(movies$gross[movies$country == "USA"], movies$gross[movies$country != "USA"], alternative = "greater"), caption="")
```

In summary, we are confident to say that the English movies in this dataset are generally more popular than non-English movies.  

The huge success of English and USA movies is not surprising. With the growing soft power of USA and UK in 20th-century, more and more worldwide audiences started to watch English movies. Some highly successful Hollywood movies are even dubbed in other languages (e.g. Chinese, Japanese, Hindi) for the consumption of non-English speaking movie-goers. On the contrary, non-English movies tend to be primarily targeted at their respective local markets.  

As a result, there are much more USA and English movies made and they are generally more successful (in terms of gross revenue) than non-English movies.  


## Content Rating vs IMDB Score

In this section, we will look at the content rating of movies and check whether it has an impact on the gross revenue generated by movies.

```{r echo=FALSE}
# Replacing M and GP with PG
movies$content_rating[movies$content_rating == "M"] <- "PG"
movies$content_rating[movies$content_rating == "GP"] <- "PG"

# Getting the top five ratings
ratingcounts <- count(movies, 'content_rating')
ratingsort <- arrange(ratingcounts, desc(freq))
topratings <- ratingsort[1:5,]

topratedmovies <- subset(movies, content_rating %in% c("PG-13", "R", "PG"))
contentrating <- topratedmovies$content_rating
contentscore <- topratedmovies$imdb_score
contentgross <- topratedmovies$gross

ggplot(topratedmovies, aes(contentrating, contentgross, fill = contentrating)) + 
    geom_boxplot() + stat_summary(fun.y=mean, colour="red", geom="point", size=1, show.legend = FALSE) +
    labs(title = "Gross Revenue of movies by rating", x = "Content Rating", y = "Gross Revenue") +
    theme_bw() +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    theme(legend.position = "none", axis.ticks.x = element_blank(), panel.grid.major.x = element_blank()) +
    annotation_logticks(sides = "lr", colour = "gray") 

# Does the gross revenue vary with different ratings?

pander(t.test(movies$gross[movies$content_rating == "R"], movies$gross[movies$content_rating == "PG-13"]), caption="")

pander(t.test(movies$gross[movies$content_rating == "PG-13"], movies$gross[movies$content_rating == "PG"]), caption="")

pander(t.test(movies$gross[movies$content_rating == "R"], movies$gross[movies$content_rating == "PG"]), caption="")
```

We conducted t-tests to test whether the mean revenue across the top three ratings - PG, PG-13 and R - is the same. 

A glance at the boxplots: R rated movies tend to, on average, generate lower revenues than PG or PG-13 rated movies. Most PG and PG-13 rated movies make between 10 and 100 million, while almost 50% of R rated movies make less than 10 million.

T-test results: There is no difference in the gross revenue for PG and PG-13 movies whereas p-values lower than 5% support the alternative hypothesis that the difference in mean gross revenue for R and PG/PG-13 movies is not equal to 0.

*The t-test result tables came one after another, it may be hard to discern which group they are comparing against. Might want to add wording in between the tables. --Siow Meng*

### Cecilia

Intuitively, the higher the budget of a movie, the better it should be and hence should attract more people to watch it, resulting in a higher gross revenue. 
We would like to investigate whether the imdb score of movie is affected by the budget for the particular movie.

*Should we move IMDB score portions to appendix?*

```{r echo = FALSE, fig.width = 10, fig.align = "center"}
#scatter plot for budget vs gross
ggplot(data = movies, aes(x = budget, y = gross)) + 
    geom_jitter(alpha = 0.2, colour = "#FF9999") + 
    theme_bw() + 
    labs(x = "Budget", y = "Gross Revenue", title = "Budget versus Gross Revenue") +
    scale_x_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    annotation_logticks(sides = "tlbr", colour = "gray") 

#covariance test
cov(movies$budget, movies$gross)

#correlation test
pander(cor.test(movies$budget, movies$gross), caption="")
```

The covariance value of 1.6 x 10^15 suggests that there is an upward trend - as budget increases, revenue from the movie increases. The Pearson's correlation test between budget and gross gives us a value of 0.22, which suggests that there is a positive correlation between the budget of a movie and its gross revenue, but the correlation itself is not very strong. Under the confidence level of 95%, the p-value is significant (2.2e-16) and hence, we can reject the null hypothesis that the true correlation is not equal to 0.

If we subset the data by different countries, we still do not see a strong correlation between budget versus gross revenue and cannot see a linear relationship between the two variables. However, a point to note are the limited number of movies from other countries, which is another limiting factor for us to assess whether the investigated relationship is apparent in different countries.

```{r echo = FALSE, fig.width = 10, fig.align = "center"}
ggplot(budgetcountry, aes(x = budget, y = gross, colour = country)) + 
    geom_jitter(alpha = 0.2) +  
    labs(title = "Budget and Gross Revenue for different countries", x = "Budget", y = "Gross Revenue") +       
    theme_bw() + 
    scale_x_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) +
    annotation_logticks(sides = "tlbr", colour = "gray") +
    facet_wrap(~ country) +
    theme(legend.position = "None")
```

### George

In case of nominal variables it doesn't make sense to talk about what happens if these variables increase/decrease, because they don't have a numerical value that can go up/down. So we can correlate neither "Genres" nor "Keywords" with "Gross Revenue" or "IMDB Score". However, there are measures of strength of association we can use that are somewhat analogous. 

*Genres analysis*

For gross revenue prediction.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
genres_model_gross <- lm(gross ~ genre, data = movies_and_genres)
#summary(genres_model_gross)
intercept1 <- lm(movies_and_genres$gross ~ genres_model_gross$fitted)$coefficients[[1]]

movies_and_genres_full <- data.frame(movies_and_genres[,c("gross","score","genre")], predicted_gross = genres_model_gross$fitted)
avg_gross <- movies_and_genres_full[, 3:4]
avg_gross <- avg_gross[!duplicated(avg_gross$genre), ]
avg_gross <- avg_gross[order(avg_gross$predicted_gross), ]
rownames(avg_gross) <- 1:dim(avg_gross)[1]

ggplot() + 
    geom_point(aes(x = movies_and_genres_full$predicted_gross, y = movies_and_genres_full$gross, colour = movies_and_genres_full$genre), alpha = 0.1) + 
    geom_abline(intercept = intercept1) + 
    geom_point(aes(x = avg_gross$predicted_gross, y = avg_gross$predicted_gross, colour = avg_gross$genre), 
               size = 2, shape = 21, stroke = 2) + 
    geom_text(aes(x = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  y = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  label = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$genre), 
                  check_overlap = F, nudge_y = -0.2, nudge_x = 0, size = 3, angle = 45, fontface = "bold") + 
    geom_text(aes(x = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  y = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  label = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$genre), 
                  check_overlap = F, nudge_y = 0.2, nudge_x = 0, size = 3, angle = 45, fontface = "bold") + 
    scale_x_continuous(breaks = c(2e+07, 4e+07, 6e+07, 8e+07, 1e+08), 
                  labels = c("20 millions", "40 millions", "60 millions", "80 millions", "100 millions")) + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) + 
    labs(title = "Gross Revenue for different genres", x = "Predicted Gross Revenue", y = "Observed Gross Revenue") +
    theme_bw() + 
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 
```

*Keywords analysis*

For gross revenue prediction.

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}
keywords_model_gross <- lm(gross ~ keywords, data = movies_and_keywords)
#summary(keywords_model_gross)
intercept3 <- lm(movies_and_keywords$gross ~ keywords_model_gross$fitted)$coefficients[[1]]

movies_and_keywords_full <- data.frame(movies_and_keywords, predicted_gross = keywords_model_gross$fitted)
avg_gross <- movies_and_keywords_full[, 3:4]
avg_gross <- avg_gross[!duplicated(avg_gross$keywords), ]
avg_gross <- avg_gross[order(avg_gross$predicted_gross), ]
rownames(avg_gross) <- 1:dim(avg_gross)[1]

ggplot() + 
    geom_point(aes(x = movies_and_keywords_full$predicted_gross, y = movies_and_keywords_full$gross, 
                   colour = movies_and_keywords_full$keywords), alpha = 0.1) + 
    geom_abline(intercept = intercept3) + 
    geom_point(aes(x = avg_gross$predicted_gross, y = avg_gross$predicted_gross, colour = avg_gross$keywords), 
               size = 2, shape = 21, stroke = 2) + 
    geom_text(aes(x = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  y = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  label = avg_gross[seq(1, dim(avg_gross)[1], 2), ]$keywords), 
                  check_overlap = F, nudge_y = -0.2, nudge_x = 0, size = 3, angle = 45, fontface = "bold") + 
    geom_text(aes(x = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  y = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$predicted_gross, 
                  label = avg_gross[seq(2, dim(avg_gross)[1], 2), ]$keywords), 
                  check_overlap = F, nudge_y = 0.2, nudge_x = 0, size = 3, angle = 45, fontface = "bold") + 
    scale_y_log10(breaks = c(1, 1e+02, 1e+04, 1e+06, 1e+08, 1e+10), 
                  labels = c("1", "100", "10,000", "1 million", "100 millions", "10 billions"), limits = c(1, 1e+10)) + 
    scale_x_continuous(breaks = c(0.3e+08, 0.4e+08, 0.5e+08, 0.6e+08, 0.7e+08), 
                  labels = c("30 millions", "40 millions", "50 millions", "60 millions", "70 millions")) +
    labs(title = "Gross Revenue for different keywords", x = "Predicted Gross Revenue", y = "Observed Gross Revenue") +
    theme_bw() + 
    theme(legend.position = "None") +
    annotation_logticks(sides = "lr", colour = "gray") 
```

When we actually make predictions based on a linear model for categorical data, the predicted value is equal to the mean of all observations for each categorical value. So, for each genre or keyword, the predicted value of the gross revenue is equal to the mean value of gross revenues of all the movies of that genre or of that keyword, e.g. the predicted gross revenue for a "Drama" movie is equal to the mean of gross revenues of all the observed "Drama" movies.

*To discuss, I don't quite understand the intercept1 and intercept3. --Siow Meng*

### IMDB Scores in North America and Europe

*To discuss whether to move IMDB scores section to appendix. --Siow Meng*

```{r, echo = FALSE, fig.width = 10, fig.align = "center"}

# code to create continent column
movies$continent <- countrycode(as.character(movies$country), "country.name", "continent")

# divide "Americas" into "North Americas" and "South America"
south_America = c("Brazil", "Argentina", "Chile", "Colombia", "Peru")

for (i in 1:nrow(movies)) {
  if (is.na(movies$continent[i])) {
    next
  } else if ((movies$continent[i] == "Americas") & (movies$country[i] %in% south_America)) {
    movies$continent[i] <- "South America"
  } else if ((movies$continent[i] == "Americas") & (!(movies$country[i] %in% south_America))) {
    movies$continent[i] <- "North America"
  } 
}
```

```{r echo=FALSE, message=FALSE, fig.width = 5, fig.height=3.5, fig.align = "right", out.extra='style="float:right"'}
# make dataframes for North America and Europe
North_America <- movies[movies$continent == "North America",]
Europe <- movies[movies$continent == "Europe",]

# separate IMDB scores 
North_America <- North_America[, c("imdb_score", "continent")]
Europe <- Europe[, c("imdb_score", "continent")]

# combine two data frames
comb <- rbind(Europe, North_America)

# plot both densities
ggplot(comb, aes(x = imdb_score)) +
    geom_density() +
    geom_density(aes(x = imdb_score, color = continent)) +
    theme_bw() +
    scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), limits = c(1, 10)) +
    labs(title = "", x = "IMDB Score", y = "Density") +
    theme(legend.title = element_blank())
```

Hollywood is arguably the biggest and most successful movie industry in the world. But monetary succes does not necessarily imply superior quality. To illustrate this tradeoff we can think of two companies, one large multinational and one medium sized company. While the large multinational can out-produce the medium sized company, its products may very well fall short on quality.

Here, we would like to test if north american movies, which are moslty made in Hollywood, differ in quality compared to european ones. Although we have no reason to believe they moght be different, the above example can serve as a guide for intuition. In order to make the comparison, IMDB scores will be considered a proxy for movie quality.

An inital look at the distribution of IMDB scores of north american and european movies reveals that european movies have a larger number of high scoring IMDB movies. The mean IMDB score for european movies is `r mean(comb[comb$continent == "Europe", "imdb_score"], na.rm = TRUE)` compared to `r mean(comb[comb$continent == "North America", "imdb_score"], na.rm = TRUE)` for north american movies. The combined mean of both continents is `r mean(comb$imdb_score, na.rm = TRUE)`.

To test whether this difference can be attributed to sampling we will use a t-test. Our null-hypothesis is that there is no difference in IMDB scores between north american and european movies.


```{r echo=FALSE}
pander(t.test(Europe$imdb_score, North_America$imdb_score), caption="")
```

With a p-value of `r t.test(Europe$imdb_score, North_America$imdb_score)[["p.value"]]`, it is highly unlikely that to randomly get a mean IMDB score of `r mean(comb[comb$continent == "Europe", "imdb_score"], na.rm = TRUE)` for european movies. The 95% confidence interval is `r t.test(Europe$imdb_score, North_America$imdb_score)[["conf.int"]][1]` - `r t.test(Europe$imdb_score, North_America$imdb_score)[["conf.int"]][2]`. We therefore reject the hypothesis that there is no difference in IMDB scores between north american and european movies. 

### Director relationship with Gross Revenue


```{r echo=FALSE, message=FALSE, fig.width = 5, fig.height=3.5, fig.align = "right", out.extra='style="float:right"'}
##plot for gross revenue vs # of movies
ggplot(movieswithdirectordata, aes(x = dir_number_of_movies, y = gross)) + 
    geom_jitter(alpha = 0.2, width = 1) + 
    geom_smooth() +
    theme_bw() +
    labs(x = "Number of movies per director", y = "Gross Revenue") + scale_y_continuous(label = human_usd)
```

To analyse the impact of the director (a proxy for how prolific the director is), on gross revenue, this section explores the relationship between director-specific variables, and the target outcome variable, gross revenue.

The basic relationship between the two variables can be seen on the figure to the right, and seems to show on upward trend for directors who have a low number of movies in the sample (roughly fewer than 10), and a less clear trend for directors who have more movies in the sample.

Two correlation tests are performend (output below) to test two hypotheses: if the number of movies per director is correlated with the gross revenue (where the correlation is `r cor(movieswithdirectordata$gross, movieswithdirectordata$dir_number_of_movies)`), and specifically if the number of movies per director is correlated with the gross revenue, for movies where the director has fewer than 10 movies (where the correlation is `r cor(movieswithdirectordata$gross[movieswithdirectordata$dir_number_of_movies<10], movieswithdirectordata$dir_number_of_movies[movieswithdirectordata$dir_number_of_movies<10])`).

```{r echo=FALSE}
##cor test for gross score vs # of movies
pander(cor.test(movieswithdirectordata$gross, movieswithdirectordata$dir_number_of_movies), caption="Correlation Test for Gross Revenue and # of movies per director in sample") 

##cor test for gross score vs # of movies for <10 movies
pander(cor.test(movieswithdirectordata$gross[movieswithdirectordata$dir_number_of_movies<10], movieswithdirectordata$dir_number_of_movies[movieswithdirectordata$dir_number_of_movies<10]), caption="Correlation Test for Gross Revenue and # of movies per director in sample /n for directors with <10 movies") 
```

These correlation tests do infer that there is a slight positive correlation between the # of movies the director has in the sample and the outcome variables, gross revenue. This correlation is both stronger and more statistically significant for directors with fewer than 10 movies in the sample. There directors with 10+ movies in the sample can be seen below:
\
\

```{r echo=FALSE, warning=FALSE, fig.width=8}
ggplot(sorteddirectorsummary[sorteddirectorsummary$number_of_movies >= 10,], aes(x = director_name, y = number_of_movies, fill = director_name)) + geom_bar(stat = "identity") + coord_flip() + guides(fill = FALSE) + labs(x = "", y = "Number of movies per director", title = "Top 20 directors by number of movies in sample") +  scale_y_continuous(expand = c(0,0)) +  expand_limits(y = c(0,1.05 * max(sorteddirectorsummary$number_of_movies))) + theme_few()
```

*To discuss whether to use logarithmic scale for below boxplot. --Siow Meng*

```{r echo=FALSE,  message=FALSE, fig.width = 5, fig.height=3.5, fig.align = "left", out.extra='style="float:left"'}
ggplot(movieswithdirectordata, aes(x = factor(more_than_ten_movies), y = gross, fill = factor(more_than_ten_movies))) + 
    geom_boxplot() + stat_summary(fun.y = mean, colour = "darkblue", geom = "point", size = 1, show.legend = FALSE) +
    theme_bw() +
    labs(x = "", y = "Gross", title = "Gross revenue distribution by \n number of movies per director") +
    scale_y_continuous(label = human_num) + 
    theme(axis.title.x = element_blank(), axis.ticks.x = element_blank(), legend.position = "None", panel.grid.major.x = element_blank()) +
    scale_x_discrete(labels = c("Fewer than 10", "10+"))
```

If we look at the categorical variable of whether or not a director has directed >10 movies, then we can see these distributions look slightly different (see figure on left).

The plots seem to suggest that the movies where the director has 10+ movies in the sample have a higher mean and a lower variance of gross revenue. However, this may be largely explained by the number of observations in each sample.

To determine whether the difference is statistically significant we run a t test and an f test (output below), to determine whether the mean is higher for movies with directors who have directed 10+ movies, than others, and whether the variance is the same for those movies.

The data suggests that the average gross revenue is higher for movies where the director has over 10 movies in the sample - which gives evidence to the claim that prolific directors are produce more popular movies (if we see gross revenue as a proxy for popularity).

Also, the opposite of the initial intuition appears to be the case for variance, the movies made by directors with over 10 movies in the sample has higher variance than others. This can be explained by the much larger number of movies made by directors with fewer than 10 movies.

```{r echo = FALSE,  message=FALSE, fig.width = 10, fig.align = "center"}
t <- t.test(movieswithdirectordata$gross[movieswithdirectordata$more_than_ten_movies=="10+"], movieswithdirectordata$gross[movieswithdirectordata$more_than_ten_movies=="Fewer than 10"], alternative="greater")

pander(t, caption="T-test of means - Gross revenue for movies with directors with more or fewer than 10 movies") 
```

```{r echo = FALSE,  message=FALSE, fig.width = 10, fig.align = "center"}
v <- var.test(movieswithdirectordata$gross[movieswithdirectordata$more_than_ten_movies=="10+"], movieswithdirectordata$gross[movieswithdirectordata$more_than_ten_movies=="Fewer than 10"])

pander(v, caption="F-test of variance - Gross revenue for movies with directors with more or fewer than 10 movies") 
```


# Predictive Data Analysis

## Model Building and Justification

Multiple models were created to predict gross revenue based on input parameters. Two models, namely Full model and English model, were built using all data available, to determine which was the most effective when tested on the test set, and one "known" model was built based on variables that would normally be available *before* a movie is released.

The results shown are when the data is trained on all data in the training set, and also when the model is trained only on movies that were released post-2010 in the training set. This is because some key variables in the dataset (i.e. the facebook like variables) became relevant only since the widespread use of Facebook, and therefore change the values of the coefficients.

## First full model

```{r echo=FALSE}
##Data Preparation for model building

##create a summary for directors with count of instances in the sample
traindirectorsummary <- ddply(train, ~ director_name,summarise, number_of_movies=length(director_name))
##add a categorical variable for ten movies or more
traindirectorsummary$more_than_ten_movies <- rep("Fewer than 10", nrow(traindirectorsummary))
traindirectorsummary$more_than_ten_movies[traindirectorsummary$number_of_movies>10] <- "10+"
trainwithdirectordata <- merge(train, traindirectorsummary[,c("director_name","more_than_ten_movies")], by="director_name")
##set factors so that Fewer than Ten is the default variable, and 10+ will show in the model
trainwithdirectordata$more_than_ten_movies <- factor(trainwithdirectordata$more_than_ten_movies, levels=c("Fewer than 10", "10+"))
trainwithdirectordata$PG_13 <- "Not PG-13"
trainwithdirectordata$PG_13[trainwithdirectordata$content_rating=="PG-13"] <- "PG-13"
trainwithdirectordata$PG_13 <- factor(trainwithdirectordata$PG_13, levels=c("Not PG-13", "PG-13"))
## merge with genre data based on title_year
## column subset 3:26 is to remove duplicating the score and gross columns)
trainwithdirectorandgenredata <- merge(movies_and_genres_wide[3:26], trainwithdirectordata, by="movie_title")
```

_Model summary_

<center>
```{r echo = FALSE, results = "asis", out.extra='style="float:left"'}
fullmodel <- lm(gross ~ budget + num_voted_users + PG_13 + duration + cast_total_facebook_likes + director_facebook_likes + Animation + Family + Mystery + Drama + num_user_for_reviews,  data=trainwithdirectorandgenredata)

fullmodel2010 <- lm(gross ~ budget + num_voted_users + PG_13 + duration + cast_total_facebook_likes + director_facebook_likes + Animation + Family + Mystery + Drama + num_user_for_reviews,  data=trainwithdirectorandgenredata[trainwithdirectorandgenredata$title_year >=2010,])

stargazer(list(fullmodel, fullmodel2010), type = "html", align = TRUE, column.labels=c("For all years", "From 2010 onwards"),
          dep.var.labels = "Gross revenue", no.space = TRUE, title = "Linear Regression results")

```
</center>

###Justification

The first full model is built using training set. The left column is trained using all training data, and the right column uses data for movies created post 2010. This is because some of our predictive variables (e.g. facebook likes) are only relevant as predictive variables since facebook has become widely popular for celebrities and movies.

####Variables included in the model:
* Inferential analysis showed that the budget of a movie is positively correlated with the gross revenue, which can be interpreted as movie-makers being wise enough to get a high return on investment.
* Although IMDB score can be seen as the "quality" of a movie, as voted on by the public. At first look this variable is positively correlated with the gross revenue, but interestingly the coefficient for this variable is negative once the "number of users voted" variable is included. An interpretation of this is that the popularity of a movie is more fully explained by the number of people reviewing the movie, and after this is taken into account, the movies with a higher "quality", as measured by imdb score, actually do less well in the box office. This could be because people are more likely to go and vote on imdb for movies they liked, and also for the obvious reason that people are more likely to go and vote on imdb for movies they have seen, so number of user votes will be linked to number of cinema tickets sold, which is linked to gross revenue. If we include number of voted users then the imdb score is statistically insignificant so is removed from the analysis.
* Inferential analysis also showed that for some ratings (e.g. PG-13) there was some impact on gross revenue, so rating has been included in the model to include these effects.
* The duration, when included into the model has a statistically significant positive coefficient, this could be interpreted as movie-goers perceiving longer movies as more value for money or higher quality.
* Cast total facebook likes can also be seen as a potential predictor for gross revenue, as in theory the more popular the actor (measured by facebook likes), the more people will go to see the movie to see that actor, the more tickets are sold and therefore the higher the revenue. We can see from the positive coefficient that this seems to be reflected in the data.
* Director facebook likes is statistically significant when included into the model, interestingly with a negative coefficient. Again, this variable is at first glance positively correlated with the gross revenue, however once you take into account the number of voted users, this relationship is reversed, potentially for similar reasons to the above.
* Key genres have been included into the model if they had statistically significant predictive power, there is a risk of overfitting here, so the variables that have been included are those that have a logical intuition, for example animated and family films are possibly more likely to be targeted to a younger audience, which may therefore have larger potential viewership, and Drama movies are the   
* The number of users for review has been included as it is statistically significant and may be correlated due to the two possible mechanisms that reviews may encourage more people to watch the movie, or the more people that see the movie, the more reviews are left.

*Unfinished sentence for the second last bulleted point: "and Drama movies are the..." --Siow Meng*

####Not included into the model:
* Movie title, director names and actor names were not included into the variable as there many different values for these variables with a small number of observations (usually one observation) for each.
* Plot keywords were not included as there was a high number of possible values, and they were not statistically significant when included into the model.
* Number of critic reviews is quite highly correlated with the number of user reviews, with a correlation coefficient of `r round(cor(movies$num_user_for_reviews,movies$num_critic_for_reviews),2)`. So it was decided to include only one of these variable into the model, in this case number of user reviews.
* When decade is included then none are found to be statistically significant, after other variables are included.

```{r echo=FALSE, fig.width = 3, fig.height=2.5, out.extra='style="float:right"'}
ggplot(movies, aes(x=color)) + geom_bar() + xlab("")
```

* When colour is included into the model then none of the options are found to be statistically significant, there is also not a very large variation in this variable in the sample set (seen right).
* When either language or country are included into the model, the R squared does go up, but by a small amount, and this also includes a lot of additional values to be interpreted, so the model loses some interpretability. Due to this, these factors have been left out. 
* The multiple actor "facebook likes" variables are mostly correlated with each other, as can be seen from the correlation matrix, so the total cast facebook likes has been chosen as the "overall" variable to explain the effects of facebook popularity of the cast.
* The number of faces in the poster has not been included into the model, when included it is only slightly statistically significant, and the intuition behind this variable is unclear, so including it may lead to model overfitting.
* IMDB score as discussed above.
* Director number of movies, although significant individually, when other variables are taken into account loses its statistical significance, so is omitted from the model.

## Second Model (English)

_Model summary_

<center>
```{r linearmodel, echo=FALSE, results="asis"}
##Data preparation
trainwithdirectorandgenredata$us_or_others <- trainwithdirectorandgenredata$country
trainwithdirectorandgenredata$us_or_others[trainwithdirectorandgenredata$country != "USA"] <- "Non-USA"
trainwithdirectorandgenredata$english <- factor(trainwithdirectorandgenredata$language == "English", levels = c(TRUE, FALSE), labels = c("English", "Non-English"))

##Linear Model
englishmodel <- lm(gross ~ budget + movie_facebook_likes + cast_total_facebook_likes + director_facebook_likes 
              + imdb_score + PG_13 + Action + Adventure + Animation + Documentary + Fantasy + Family 
              + us_or_others + english, 
              data = trainwithdirectorandgenredata)

englishmodel2010 <- lm(gross ~ budget + movie_facebook_likes + cast_total_facebook_likes + director_facebook_likes 
                  + imdb_score + PG_13 + Action + Adventure + Animation + Documentary + Fantasy + Family 
                  + us_or_others + english, 
                  data = trainwithdirectorandgenredata[trainwithdirectorandgenredata$title_year >= 2010, ])

stargazer(list(englishmodel, englishmodel2010), type = "html", align = TRUE, 
          column.labels=c("For all years", "From 2010 onwards"),
          dep.var.labels = "Gross revenue", no.space = TRUE, title = "Linear Regression results")
```
</center>

###Justification

The second model builds on the observation that English and USA movies are generally more successful than others, and uses this information as predictor variables. We will name this model the "English" model here.  

Similar to the first full model, this model is built using training set. The left column is trained using all training data, and the right column uses data for movies created post 2010. This is because some of our predictive variables (e.g. facebook likes) are only relevant as predictive variables since facebook has become widely popular for celebrities and movies.  

####Variables included in the model:  

Some of the variables used in this model are identical to those used in the first model. The justifications for these variables can be found in earlier section. The list below discusses the variables used only in the second model:  

* Two new variables ("english" and "us_or_others") have been created to separate the movies into two distinct groups (i.e. English vs non-English, USA vs non-USA). From the inferential analysis section, we have seen a huge difference in gross revenue between English and non-English (similarly, USA and non-USA). Therefore, these two variables are expected to have a major effect in predicting gross revenue.
* This model does not consider the "number of users voted" variable. Intuitively, higher number of voters does not necessarily translate to higher movie quality. IMDB score may reflect the movie quality better instead. In the descriptive analysis section, we have observed that there is a slight positive correlation between gross revenue and IMDB score. This model includes IMDB score to predict the quality and popularity of the movie.
* Movie facebook likes is also statistically significant when included into the model, this can be interpreted as a proxy for the popularity of the movie, and also could be that the more people who saw the movie (based on gross revenue), the more people then subsequently liked the page.
* From the genre analysis section, it can be seen that Documentaries have signficantly lower gross revenue while Action, Adventure, Animation, Family and Fantasy movies are more popular than the rest. These six genres are included in the model.

####Not included into the model:  
We have already discussed some of the variables not included in this model (in the section for first model), the list below describes the rationale in excluding some other variables:  

* Number of voted users is not included as explained above.
* Number of critic reviews is not included. Similar to reasons in excluding "number of voted users", this metric is not considered to have a major impact on the popularity of a movie.
* Movie duration is excluded since it should not affect the quality (and hence the popularity) of a movie.

##Third Model, with only pre-known variables

_Model summary_

<center>
```{r echo=FALSE, results = "asis"}
knownvariablesmodel <- lm(gross ~ budget + PG_13 + duration + cast_total_facebook_likes + director_facebook_likes + Animation + Family + Mystery + Drama + more_than_ten_movies, data=trainwithdirectorandgenredata)

knownvariablesmodel2010 <- lm(gross ~ budget + PG_13 + duration + cast_total_facebook_likes + director_facebook_likes + Animation + Family + Mystery + Drama + more_than_ten_movies, data=trainwithdirectorandgenredata[trainwithdirectorandgenredata$title_year>=2010, ])

stargazer(list(knownvariablesmodel, knownvariablesmodel2010), type = "html", align = TRUE, column.labels=c("For all years", "From 2010 onwards"), 
          dep.var.labels = "Gross revenue", no.space = TRUE, title = "Linear Regression results")
```
</center>

###Justification

To build a model to predict revenue of movies that have not yet been released, this model includes only variables that are known before the time of release. Our dataset provided us with number of facebook likes at a specific point in time, and to use this as part of our predictive model we would need to analyse number of facebook likes before the movie was released, however we will use what we currently have as a proxy to build the model. This model has lower predictive power (adjusted R squared of `r summary(knownvariablesmodel)$adj.r.squared`), but could be used in a more useful business context.

##Model Performance

*we will then test this model using the test dataset, make a plot of the predictions against the actual values, and calculate the MSE/other success statistics*

Having built the three models, we will now use the test set to test their performances. The models trained using all movies data in the training set are tested against the whole test set, whereas the models trained using post-2010 training data are tested against the post-2010 data in the test set.  

```{r conditiontestdata, echo=FALSE}
##Data Preparation for testing

##create a summary for directors with count of instances in the sample
testdirectorsummary <- ddply(test, ~ director_name,summarise, number_of_movies=length(director_name))
##add a categorical variable for ten movies or more
testdirectorsummary$more_than_ten_movies <- rep("Fewer than 10", nrow(testdirectorsummary))
testdirectorsummary$more_than_ten_movies[testdirectorsummary$number_of_movies>10] <- "10+"
testwithdirectordata <- merge(test, testdirectorsummary[,c("director_name","more_than_ten_movies")], by="director_name")
##set factors so that Fewer than Ten is the default variable, and 10+ will show in the model
testwithdirectordata$more_than_ten_movies <- factor(testwithdirectordata$more_than_ten_movies, levels=c("Fewer than 10", "10+"))
testwithdirectordata$PG_13 <- "Not PG-13"
testwithdirectordata$PG_13[testwithdirectordata$content_rating=="PG-13"] <- "PG-13"
testwithdirectordata$PG_13 <- factor(testwithdirectordata$PG_13, levels=c("Not PG-13", "PG-13"))
## merge with genre data based on movie_title
## column subset 3:26 is to remove duplicating the score and gross columns)
test <- merge(movies_and_genres_wide[3:26], testwithdirectordata, by="movie_title")

test$english <- factor(test$language == "English", levels = c(TRUE, FALSE), labels = c("English", "Non-English"))
test$us_or_others <- factor(test$country == "USA", levels = c(TRUE, FALSE), labels = c("USA", "Non-USA"))
```

The Root-Mean-Square Error of the models are tabulated below.  

```{r prediction, echo = FALSE, results = TRUE}

##calculate the prediction and RMSE for all 3 models using all data and data post 2010
fullmodelpre <- predict(fullmodel, test)
fullmodelrmse <- sqrt(mean((fullmodelpre - test$gross)^2))
englishmodelpre <- predict(englishmodel, test)
englishmodelrmse <- sqrt(mean((englishmodelpre - test$gross)^2))
knownvariablesmodelpre <- predict(knownvariablesmodel, test)
knownvariablesmodelrmse <- sqrt(mean((knownvariablesmodelpre - test$gross)^2))

fullmodel2010pre <- predict(fullmodel2010, test[test$title_year >= 2010, ])
fullmodel2010rmse <- sqrt(mean((fullmodel2010pre - test$gross[test$title_year >= 2010])^2))
englishmodel2010pre <- predict(englishmodel2010, test[test$title_year >= 2010, ])
englishmodel2010rmse <- sqrt(mean((englishmodel2010pre - test$gross[test$title_year >= 2010])^2))
knownvariablesmodel2010pre <- predict(knownvariablesmodel2010, test[test$title_year >= 2010, ])
knownvariablesmodel2010rmse <- sqrt(mean((knownvariablesmodel2010pre - test$gross[test$title_year >= 2010])^2))

##turn RMSE data into a table for display with columns of "all data" and "post 2010", and rows of model type
rmsetable <- data.frame(alldata=c(fullmodelrmse, englishmodelrmse, knownvariablesmodelrmse), post2010 = c(fullmodel2010rmse, englishmodel2010rmse, knownvariablesmodel2010rmse))
rownames(rmsetable) <- c("Full Model", "English Model", "Known Variables Model")
colnames(rmsetable) <- c("Model using all data", "Model using post 2010 data")
pander(rmsetable, format="markdown")

```

For each test scenario, we included two plots:  

1. Scatter plot of Actual Revenue vs Predicted Revenue: the straight line indicates the y value of the predicted revenue. The closer the points are towards the straight line, the better the performance of the prediction model.    
2. Residual plot: the horizontal line indicate zero residual value. The closer the points are towards this line, the better the performance of the prediction model.  

```{r echo=FALSE, fig.align="center"}
##Plot the predictions and residuals for full model with all data
dffullmodel <- data.frame(y_actual = test$gross, y_predict = fullmodelpre)
plot1 <- ggplot(data = dffullmodel, aes(x = y_predict, y = y_actual)) + geom_point(colour = "red") + 
    geom_abline(colour = "blue") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("Full Model All Data")
plot2 <- ggplot(data = dffullmodel, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "red") + geom_hline(yintercept = 0, colour = "blue") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 2e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("Full Model All Data")
##Plot the predictions and residuals for full model post 2010
dffullmodel2010 <- data.frame(y_actual = test$gross[test$title_year>=2010], y_predict = fullmodel2010pre)
plot3 <- ggplot(data = dffullmodel2010, aes(x = y_predict, y = y_actual)) + geom_point(colour = "blue") + 
    geom_abline(colour = "red") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("Full Model post 2010")
plot4 <- ggplot(data = dffullmodel2010, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "blue") + geom_hline(yintercept = 0, colour = "red") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 2e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("Full Model post 2010")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

The full model is the best performing linear model (with lowest RMSE). Most of the points are reasonably close to the straight lines. However, the model does overestimate the popularity of certain movies. The residual tends to increase as the predicted value increases.  

The other full model (which is trained using post-2010 data) seems to perform much better in predicting the popularity of post-2010 movies.  

```{r echo=FALSE, fig.align="center"}

##Plot the predictions and residuals for facebook model with all data
dfenglishmodel <- data.frame(y_actual = test$gross, y_predict = englishmodelpre)
englishmodelplot1 <- ggplot(data = dfenglishmodel, aes(x = y_predict, y = y_actual)) + geom_point(colour = "red") + 
    geom_abline(colour = "blue") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("English Model All Data")
englishmodelplot2 <- ggplot(data = dfenglishmodel, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "red") + geom_hline(yintercept = 0, colour = "blue") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 2e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("English Model All Data")
##Plot the predictions and residuals for facebook model post 2010
dfenglishmodel2010 <- data.frame(y_actual = test$gross[test$title_year>=2010], y_predict = englishmodel2010pre)
englishmodelplot3 <- ggplot(data = dfenglishmodel2010, aes(x = y_predict, y = y_actual)) + 
    geom_point(colour = "blue") + 
    geom_abline(colour = "red") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("English Model post 2010")
englishmodelplot4 <- ggplot(data = dfenglishmodel2010, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "blue") + geom_hline(yintercept = 0, colour = "red") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 2e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("English Model post 2010")
grid.arrange(englishmodelplot1, englishmodelplot2, englishmodelplot3, englishmodelplot4, ncol=2)
```

The English model performs worse than the full model in terms of RMSE. Although the model has more difficulties in identifying hugely successful movies, it is less prone in overestimating the gross revenues of less successful movies. Its residuals also stay within a certain range for most predicted values.  

The English model trained using post-2010 data also performs better against post-2010 test set. This is expected since some of the independent variables used (e.g. movie facebook likes) are deemed to have more predictive power for post-2010 movies.  

```{r echo=FALSE, fig.align="center"}

##Plot the predictions and residuals for known variables model with all data
dfknownvariablesmodel <- data.frame(y_actual = test$gross, y_predict = knownvariablesmodelpre)
knownvariablesmodelplot1 <- ggplot(data = dfknownvariablesmodel, aes(x = y_predict, y = y_actual)) + geom_point(colour = "red") + 
    geom_abline(colour = "blue") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("Known Variables Model All Data")
knownvariablesmodelplot2 <- ggplot(data = dfknownvariablesmodel, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "red") + geom_hline(yintercept = 0, colour = "blue") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 2e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("Known Variables Model All Data")

##Plot the predictions and residuals for known variables model post 2010
dfknownvariablesmodel2010 <- data.frame(y_actual = test$gross[test$title_year>=2010], y_predict = knownvariablesmodel2010pre)
knownvariablesmodelplot3 <- ggplot(data = dfknownvariablesmodel2010, aes(x = y_predict, y = y_actual)) + geom_point(colour = "blue") + 
    geom_abline(colour = "red") + xlab("Predicted Gross Revenue (US$)") + 
    ylab("Actual Gross Revenue(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(0, 3e8)) + 
    scale_y_continuous(label = human_usd)  + scale_x_continuous(label = human_usd) + ggtitle("Known Variables Model post 2010")
knownvariablesmodelplot4 <- ggplot(data = dfknownvariablesmodel2010, aes(x = y_predict, y = y_actual - y_predict)) + 
    geom_point(colour = "blue") + geom_hline(yintercept = 0, colour = "red") + 
    xlab("Predicted Gross Revenue (US$)") + ylab("Residual(US$)") + 
    coord_cartesian(xlim = c(-0.2e8, 2.5e8), ylim = c(-1.5e8, 1.5e8)) + 
    scale_y_continuous(label = human_usd) + scale_x_continuous(label = human_usd) + ggtitle("Known Variables Model post 2010")
grid.arrange(knownvariablesmodelplot1, knownvariablesmodelplot2, knownvariablesmodelplot3, knownvariablesmodelplot4, ncol=2)
```

The last model, known variable model, left out some independent variables that are known only after the movie release (e.g. movie facebook likes, IMDB score). Consequently, it is expected to be less accurate in predicting the gross revenue. As expected, it is the worst performing model. It is less accurate in identifying highly successful movies and is prone to overestimate the popularity of certain movies.  

Interestingly, the accuracy of known variables model also improved when it is trained and tested against post-2010 movie data. It seem that the independent variables used in this model are able to predict the gross revenues of recent movies (i.e. post-2010) more accurately.  

#Dicussion

##Conclusion

###Limitation of Linear Models

All the three linear models predicted that certain movies would have negative gross revenues. Clearly this does not make sense since in worst case, the lowest possible revenue achieved would be zero (when absolutely nobody watches the movie in cinema).  

This is the limitation of our linear models: the independent variables, which are negatively correlated with gross revenue, might cause the gross revenue to be a negative number. In this case, we would have to be careful in interpreting the prediction results.  

###Accuracy of Predictive Models

While the full model delivers a reasonably good performance in predicting the movie popularity, the Root-Mean-Square Error is still rather high (around US$40 million). The key factors constituting a popular movie might not be captured in this Kaggle dataset. To improve the prediction accuracy, we could potentially explore other metadata of movies. For instance, we could inspect whether the movie is a remake/prequel/sequel of a popular movie, adapted from a popular novel, biography or computer game.  

##Next Steps

*to be completed after our analysis*
*need to write something interesting about our data*


